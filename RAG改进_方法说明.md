                          RAG改进

题目：
Retrieval Augmented Generation(RAG) with LLM是目前比较热门的应用之一，实现并不难，但提取内容的准确度是普遍存在的问题。想要提高准确度，需要考虑多个细节，例如:如何保证文档切片不会造成相关内容丢失，切片大小如何控制，如何保证召回内容跟问题是相关的等等。请提供相关的代码实现，尽可能的解决RAG准确度低的问题。
Key function
●Langchain已经提供了一些api接口，可以调用，但需要写明白解决了哪方面的问题，同时也应该有自己的改进
·提供一个demo，去展示该方案使用前后的效果对比，给出准确度定量的估计，不少于5个例子·
加分:Tree-of-Througt, Graph-of-Throught, Knowledge-Graph


Azure OpenAl
endpoint: https://oh-ai-openai-scu.openai.azure.com/ 
key: 6f7094c03b3e448fac43473fde475a47 
deployment: gpt-35-turbo 
model: gpt-35-turbo 
api_type: azure
api_version: 2023-05-15






RAG改进
RAG是基于知识库的应用，一般分为两块。第一块是文档处理，另一块
是文档召回（Retriever）。
文档处理分为以下几步：
1. 文档解析 - 将上传的文档解析成文本内容
2. 文档切片 - 切成若干块（chunk），以便于后续的召回
3. 向量存储 - 利用嵌入模型将文本转换成向量，存放在向量数据库中。
文档召回分为以下几步：
1.向量搜索 - 利用相似度算法匹配与提问内容相关文档块
2. LLM请求 - 将获取到的内容，放到LLM 对话上下文中
3. LLM返回 - LLM 根据你的上下文来回答问题

问题：
1. 如何保证文档切片不会造成相关内容的丢失？ 比如一段完整的文本，如果从中间切开，那么则会造成信息丢失，给 LLM 的内容则不完整。
2. 文档切片的大小如何控制？ 太小则容易造成信息丢失，太大则不利于向量检索命中。

解决思路：
针对文档切片大小的问题，文档切片比较小时，更容易召回相关文档。文档切片比较大时，因为有更多内容的污染，语义召回不容易召回。但是，切片较小时，容易把相关的信息丢失，造成信息不完整。为了解决召回和信息丢失的问题，我们结合两者的优点，召回时，使用较小的切片，召回的切片，我们将这个切片和相邻的切片拼接起来，形成新的切片，即将召回的切片和他前后相邻的切片拼接在一起作为召回的文本。这些新的拼接切片作为对话上下文。

实现过程：
使用比较小的块大小（chunk_size)对文件进行切片,对每一个切片添加一个块编码（chunk_idx_name），这个块编码表示这个块在所有块中的次序。将切片向量化后存入矢量数据库。
在进行QA问答时，用户输入问题query,将query向量化和所有的块进行相似度比较，召回最相关的块。召回的切片通过merge_chunk（）函数，把这个切片和它的前后切片拼接起来，形成新的切片内容。把query 和这些新的切片内容一起送入问答系统，并返回回答的内容。


实验结果：
我们新的想法实现在文件rag_qa_new.py中，作为对比的程序文件为rag_qa.py，使用的外部文件为shenzhen.txt。
我们使用了6个测试案例，其中rag_qa_new.py回答完全正确。rag_qa.py正确2个，错误4个。可见，这样的一个解决方案有一定的有效性。